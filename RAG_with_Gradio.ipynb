{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8fcea0b-a57b-420e-a210-f77c17d32050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7954\n",
      "* Running on public URL: https://4562247a1cd99dd4f8.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4562247a1cd99dd4f8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import docx\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import requests  # For making HTTP requests to Ollama API\n",
    "from ollama import Client\n",
    "\n",
    "\n",
    "# Initialize Ollama client\n",
    "client = Client(\n",
    "  host='http://localhost:11434',\n",
    "  headers={'x-some-header': 'some-value'}\n",
    ")\n",
    "\n",
    "# Function to handle file upload and return the file path\n",
    "def upload_file(file):\n",
    "    UPLOAD_FOLDER = \"./RAG_data\"\n",
    "    if not os.path.exists(UPLOAD_FOLDER):\n",
    "        os.mkdir(UPLOAD_FOLDER)\n",
    "    \n",
    "    # Save the uploaded file to the destination folder\n",
    "    destination = os.path.join(UPLOAD_FOLDER, os.path.basename(file.name))\n",
    "    shutil.copy(file.name, destination)\n",
    "    \n",
    "    # Return the file path for further processing\n",
    "    return destination\n",
    "\n",
    "# Function to extract text from a PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        reader = PyPDF2.PdfReader(pdf_path)\n",
    "        text = \"\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting text from PDF: {str(e)}\"\n",
    "\n",
    "# Function to extract text from Word documents\n",
    "def extract_text_from_docx(docx_path):\n",
    "    try:\n",
    "        doc = docx.Document(docx_path)\n",
    "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting text from DOCX: {str(e)}\"\n",
    "\n",
    "# Function to extract text from Excel and CSV files\n",
    "def extract_text_from_spreadsheet(file_path):\n",
    "    try:\n",
    "        if file_path.endswith(\".csv\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            df = pd.read_excel(file_path)\n",
    "        return df.to_string()\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting text from spreadsheet: {str(e)}\"\n",
    "\n",
    "# Function to extract text based on file type\n",
    "def extract_text_from_file(file_path):\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif file_path.endswith(\".xlsx\") or file_path.endswith(\".xls\") or file_path.endswith(\".csv\"):\n",
    "        return extract_text_from_spreadsheet(file_path)\n",
    "    else:\n",
    "        return \"Unsupported file format. Please upload a PDF, DOCX, XLSX, XLS, or CSV file.\"\n",
    "\n",
    "# Function to split text into smaller chunks\n",
    "def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.create_documents([text])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Function to create embeddings using SentenceTransformer\n",
    "def create_local_embeddings():\n",
    "    return SentenceTransformer('all-MiniLM-L6-v2')  # Local embedding model\n",
    "\n",
    "\n",
    "def embed_chunks_locally(chunks, model):\n",
    "    chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "    embeddings = model.encode(chunk_texts, convert_to_numpy=True)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Function to index chunks with FAISS\n",
    "def index_chunks_with_faiss(chunks, embeddings):\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "\n",
    "# Function to retrieve relevant chunks from FAISS\n",
    "def retrieve_relevant_chunks(faiss_index, query, model, chunks, top_k=50):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    relevant_chunks = [chunks[i] for i in indices[0]]\n",
    "    return relevant_chunks\n",
    "\n",
    "\n",
    "# Function to interact with Ollama model (Qwen2.5:32b)\n",
    "def query_ollama(model_name, input_text):\n",
    "    try:\n",
    "        # Running the Ollama CLI command using subprocess\n",
    "        result = subprocess.run(\n",
    "            ['ollama', 'run', model_name, input_text],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            return result.stdout\n",
    "        else:\n",
    "            return f\"Error: {result.stderr}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Function to create the RAG pipeline\n",
    "def create_rag_pipeline_local(faiss_index, embedding_model, generator, chunks, model):\n",
    "    def generate_answer(query):\n",
    "        # Retrieve relevant chunks\n",
    "        relevant_docs = retrieve_relevant_chunks(faiss_index, query, embedding_model, chunks)\n",
    "        context = \"Given the context information above, I want you to think step by step to answer the query in a highly precise manner focused on the final answer, incase case you don't know the answer say 'I do not know!'\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "        \n",
    "        # Generate the answer\n",
    "        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "        return generator(model, prompt)\n",
    "\n",
    "    return generate_answer\n",
    "\n",
    "# RAG Pipeline\n",
    "def rag_pipeline(file_path, query):\n",
    "    # Extract text from file\n",
    "    file_text = extract_text_from_file(file_path)\n",
    "    if \"Error\" in file_text or file_text == \"Unsupported file type.\":\n",
    "        print(file_text)\n",
    "        return\n",
    "\n",
    "    # Split text into chunks\n",
    "    chunks = split_text_into_chunks(file_text)\n",
    "\n",
    "    # Create embeddings\n",
    "    embedding_model = create_local_embeddings()\n",
    "    embeddings = embed_chunks_locally(chunks, embedding_model)\n",
    "\n",
    "    # Index the embeddings with FAISS\n",
    "    faiss_index = index_chunks_with_faiss(chunks, embeddings)\n",
    "\n",
    "    # Create the RAG pipeline\n",
    "    rag_pipeline = create_rag_pipeline_local(faiss_index, embedding_model, query_ollama, chunks, model='qwen2.5:32b')\n",
    "\n",
    "    # Get the answer to the query\n",
    "    answer = rag_pipeline(query)\n",
    "    #print(\"Answer:\", answer)\n",
    "    return answer\n",
    "\n",
    "# Function to handle file input and update content and path\n",
    "def handle_file_input(file):\n",
    "    file_path = upload_file(file)\n",
    "    file_content = extract_text_from_file(file_path)  # Extract file content\n",
    "    return file_path, file_content\n",
    "\n",
    "# Function to handle the query after file content has been loaded\n",
    "def handle_query(file_path, query):\n",
    "    if not file_path:\n",
    "        return \"No file uploaded yet. Please upload a file first.\"\n",
    "    answer = rag_pipeline(file_path, query)  # Execute query after file is loaded\n",
    "    return answer\n",
    "\n",
    "def clear_all():\n",
    "    return None, None, None  # Reset file path, file content, and answer to default\n",
    "    \n",
    "# Gradio UI components for file upload and query input\n",
    "with gr.Blocks(css=\".center-text {text-align: center;}\") as demo:\n",
    "    # Centered Title\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            # RAG over Files\n",
    "            Upload your documents and ask a question. This system processes the file and provides answers based on its content.\n",
    "            \"\"\",\n",
    "            elem_id=\"title\",  # Custom ID for CSS styling\n",
    "        )\n",
    "\n",
    "    # File Upload\n",
    "    file_input = gr.File(label=\"Add your documents! Provide a file path (PDF, DOCX, XLSX, XLS, or CSV)\")\n",
    "    file_path_output = gr.Textbox(label=\"File Path\", interactive=False, visible=False)\n",
    "    file_content_output = gr.Textbox(label=\"File Content\", interactive=False)\n",
    "    \n",
    "    # Query Input and Answer Output\n",
    "    query_input = gr.Textbox(label=\"Query:\", placeholder=\"What's Up?\")\n",
    "    answer_output = gr.Textbox(label=\"Answer\", interactive=False)\n",
    "\n",
    "    # Clear All Button\n",
    "    clear_button = gr.Button(\"Clear All ↺\")\n",
    "    \n",
    "    # Handle file input: file upload triggers file content extraction\n",
    "    file_input.change(\n",
    "        handle_file_input,\n",
    "        inputs=[file_input],\n",
    "        outputs=[file_path_output, file_content_output]\n",
    "    )\n",
    "    \n",
    "    # Handle query input: execute query after file is loaded\n",
    "    query_input.submit(\n",
    "        handle_query,\n",
    "        inputs=[file_path_output, query_input],\n",
    "        outputs=[answer_output]\n",
    "    )\n",
    "\n",
    "    # Clear all components\n",
    "    clear_button.click(\n",
    "        clear_all,\n",
    "        inputs=[],\n",
    "        outputs=[file_path_output, file_content_output, answer_output]\n",
    "    )\n",
    "    \n",
    "# Launch Gradio UI\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43dd684-7b62-4eb5-aa33-9e4019a3fdba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rubel (Python3.11)",
   "language": "python",
   "name": "rubel3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
